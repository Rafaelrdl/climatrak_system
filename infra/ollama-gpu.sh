#!/bin/bash
# Script para gerenciar Ollama com/sem GPU
# Uso:
#   ./ollama-gpu.sh start    # Inicia com GPU
#   ./ollama-gpu.sh stop     # Para o Ollama
#   ./ollama-gpu.sh cpu      # Inicia com CPU only
#   ./ollama-gpu.sh status   # Verifica status
#   ./ollama-gpu.sh test     # Testa se GPU est√° funcionando

set -e

COMPOSE_FILE="docker-compose.yml"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

case "$1" in
    start|gpu)
        echo "üöÄ Iniciando Ollama com GPU NVIDIA..."
        docker compose --profile gpu up -d ollama-gpu
        echo "‚è≥ Aguardando Ollama iniciar..."
        sleep 5
        echo "üîç Verificando GPU..."
        docker exec climatrak-ollama nvidia-smi 2>/dev/null || echo "‚ö†Ô∏è  nvidia-smi n√£o dispon√≠vel (GPU pode n√£o estar configurada)"
        echo ""
        echo "‚úÖ Ollama iniciado! Teste com:"
        echo "   curl http://localhost:11434/api/tags"
        ;;
    
    cpu)
        echo "üñ•Ô∏è  Iniciando Ollama com CPU only..."
        docker compose --profile cpu up -d ollama
        echo "‚úÖ Ollama (CPU) iniciado!"
        ;;
    
    stop)
        echo "üõë Parando Ollama..."
        docker compose --profile gpu stop ollama-gpu 2>/dev/null || true
        docker compose --profile cpu stop ollama 2>/dev/null || true
        docker rm climatrak-ollama 2>/dev/null || true
        echo "‚úÖ Ollama parado!"
        ;;
    
    status)
        echo "üìä Status do Ollama:"
        docker ps --filter "name=climatrak-ollama" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
        echo ""
        echo "üì¶ Modelos dispon√≠veis:"
        curl -s http://localhost:11434/api/tags 2>/dev/null | jq -r '.models[].name' 2>/dev/null || echo "Ollama n√£o est√° rodando"
        ;;
    
    test)
        echo "üß™ Testando GPU no container Ollama..."
        if docker exec climatrak-ollama nvidia-smi 2>/dev/null; then
            echo ""
            echo "‚úÖ GPU NVIDIA detectada e funcionando!"
            echo ""
            echo "üî• Testando infer√™ncia com GPU..."
            time curl -s http://localhost:11434/api/generate -d '{"model":"mistral-nemo","prompt":"Say hello","stream":false}' | jq -r '.response'
        else
            echo "‚ùå GPU n√£o dispon√≠vel no container."
            echo ""
            echo "Verifique:"
            echo "  1. Driver NVIDIA instalado no host: nvidia-smi"
            echo "  2. Docker Desktop configurado com WSL2 backend"
            echo "  3. Container iniciado com --profile gpu"
        fi
        ;;
    
    pull)
        echo "üì• Baixando modelo mistral-nemo..."
        docker exec climatrak-ollama ollama pull mistral-nemo
        echo "‚úÖ Modelo baixado!"
        ;;
    
    *)
        echo "Uso: $0 {start|gpu|cpu|stop|status|test|pull}"
        echo ""
        echo "Comandos:"
        echo "  start/gpu  - Inicia Ollama com GPU NVIDIA"
        echo "  cpu        - Inicia Ollama com CPU only"
        echo "  stop       - Para o Ollama"
        echo "  status     - Mostra status e modelos"
        echo "  test       - Testa se GPU est√° funcionando"
        echo "  pull       - Baixa modelo mistral-nemo"
        exit 1
        ;;
esac
